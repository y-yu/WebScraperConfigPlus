=encoding utf8

=head1 NAME

WebScraperConfigPlus - Run Web::Scraper From Config Files ぷらす

=head1 SYNOPSIS

 my $scraper = WebScraperConfigPlus->new($config);
 
 my $result = $scraper->scrape($url);

=head1 DESCRIPTION

設定ファイルにスクレイピングルールと、その前後の処理を行うサブルーチンを記述出来るようにしてみた。

大部分はWeb::Scraper::Config(http://search.cpan.org/~dmaki/Web-Scraper-Config/)を使わせて頂いた。
有用なモジュール作成なさったDaisuke Makiさんを尊敬します。

=head1 METHOD

=head2 new

インスタンス作成。
設定ファイルのパスを入れるだけ。

Web::Scraper::Configとは違って、callbackも設定ファイルに書きこむので、
callbackを取る機能はない。

=head2 scrape

普通はURIのインスタンスか文字列のURLを放り込めばいい。

引数はWeb::Scraper::scrapeと同じ。

=head1 CONFIG FILE

YAMLで記述した例を示すが、JSONやXMLでも問題ない。
設定ファイルの読み込みにはConfig::Anyを使っているので、それが読めるならなんでもいい。

 ---
 scraper:
     - process:
         - '//table[ @summary="xxxxxx" ]/tbody'
         - 'info[]'
         - scraper:
		     - process:
                 - '//tr/td[2]'
                 - 'title'
                 -
                     - 'TEXT'
                     - sub: 'sub { s/\s//g; }'
             - process:
                 - '//tr[2]/td/a[3]'
                 - 'url'
                 - '@href'
             - process:
                 - '//tr[2]/td/a[3]'
                 - 'number'
                 -
                     - '@href'
                     - sub: 'sub { m/(\d{6})\.php/ and return $1 }'
 
 ---
 scraper:
     - process:
         - '/html/head/meta[2]'
         - 'discription'
         - '@content'
     - process:
         - '//input[ @type="hidden" ][ @name="v" ]'
         - 'id'
         -
             - '@value'
             - sub: |
                 sub {
                     use MIME::Base64 qw(decode_base64);

                     return decode_base64($_);
                 }
 
 ---
 subroutine:
     after : |
         sub {
             my $res = (shift)->[0];
 
             return $res->{info};
         }
 
 ---
 subroutine:
     before : |
         sub {
             my $result = shift;
 
             return $result;
         }
 
     after  : |
         sub { 
             my $result = shift;
 
             return $result;
         }

=head2 Syntax

=head3 scraper

スクレイピングのルール。
ほとんどWeb::Scraper::Configと同じだが、サブルーチン（フィルター）を中にぶちこめる。

scraperやprocessについては、
Web::Scraper(http://search.cpan.org/~miyagawa/Web-Scraper/)や前述のWeb::Scraper::Configを参考にするといい。

scraperを複数記述した時の処理は後述する。

=head3 subroutine

スクレイピングの前後に実行されるサブルーチンを定義出来る。

beforeで定義したサブルーチンが、スクレイピングの前に実行され、
afterで定義したサブルーチンがスクレイビングの後に実行される。

スクレイピングだけで、後述するbeforeが返す値と同じフォーマットの配列リファレンスが用意出来れば、
必ずしも用意する必要はない。

n番目に定義したsubroutineがn番目のスクレイピングの時に用いられるので、
例えば、4番目はいらないけど、5番目は欲しいといった場合には、
設定ファイルに空のsubroutineを置く必要がある。

=head4 before

与えられる引数は配列リファレンス。
一回目はWebScraperConfigPlus::scrapeに与えられた配列リファレンス、
二回目以降は、前回のafterの返り値が入る。

返り値は配列リファレンスを返すこと。

 $return = [
 	{
 		url  => 'http://example.net/' or URI instance
 		lest => HTML::Tree or []
 	},
 	{
 		...
 	},
 	...
 ];

この様な配列リファレンスを返えす。

urlを次のスクレイピングの際に使う。
これらはループで回してスクレイピングするので、複数のURLをまとめてスクレイピング出来る。

=head4 after

スクレイピング結果の配列リファレンス。
配列なので、一つ目のURLのスクレイピング結果は、$_[0]->[0]にある。

これは複数のURLをスクレイピングした結果を得たい時に便利なように、単一の時の利便性を捨てたから。

=head1 SEQUENCE

処理の流れは、

第一サブルーチンのbefore（任意）→第一スクレイピング→第一サブルーチンのafter（任意）→第二サブルーチンのbefore ……

となる。
ここで、第一、第二という序列は、設定ファイルで定義した順になる。

=head1 AUTHOR

@yoshimura_yuu

=head1 LICENSE

This program is free software; you can redistribute it and/or modify it
under the same terms as Perl itself.

See http://www.perl.com/perl/misc/Artistic.html

=cut
